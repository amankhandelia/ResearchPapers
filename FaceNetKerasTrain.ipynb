{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import add\n",
    "from keras import backend as K\n",
    "\n",
    "from PIL import Image\n",
    "from six.moves import xrange\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced from https://github.com/nyoki-mtl/keras-facenet/blob/master/code/inception_resnet_v1.py\n",
    "\n",
    "def scaling(x, scale):\n",
    "    return x * scale\n",
    "\n",
    "\n",
    "def conv2d_bn(x,\n",
    "              filters,\n",
    "              kernel_size,\n",
    "              strides=1,\n",
    "              padding='same',\n",
    "              activation='relu',\n",
    "              use_bias=False,\n",
    "              name=None):\n",
    "    x = Conv2D(filters,\n",
    "               kernel_size,\n",
    "               strides=strides,\n",
    "               padding=padding,\n",
    "               use_bias=use_bias,\n",
    "               name=name)(x)\n",
    "    if not use_bias:\n",
    "        bn_axis = 1 if K.image_data_format() == 'channels_first' else 3\n",
    "        bn_name = _generate_layer_name('BatchNorm', prefix=name)\n",
    "        x = BatchNormalization(axis=bn_axis, momentum=0.995, epsilon=0.001,\n",
    "                               scale=False, name=bn_name)(x)\n",
    "    if activation is not None:\n",
    "        ac_name = _generate_layer_name('Activation', prefix=name)\n",
    "        x = Activation(activation, name=ac_name)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _generate_layer_name(name, branch_idx=None, prefix=None):\n",
    "    if prefix is None:\n",
    "        return None\n",
    "    if branch_idx is None:\n",
    "        return '_'.join((prefix, name))\n",
    "    return '_'.join((prefix, 'Branch', str(branch_idx), name))\n",
    "\n",
    "\n",
    "def _inception_resnet_block(x, scale, block_type, block_idx, activation='relu'):\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else 3\n",
    "    if block_idx is None:\n",
    "        prefix = None\n",
    "    else:\n",
    "        prefix = '_'.join((block_type, str(block_idx)))\n",
    "    name_fmt = partial(_generate_layer_name, prefix=prefix)\n",
    "\n",
    "    if block_type == 'Block35':\n",
    "        branch_0 = conv2d_bn(x, 32, 1, name=name_fmt('Conv2d_1x1', 0))\n",
    "        branch_1 = conv2d_bn(x, 32, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n",
    "        branch_1 = conv2d_bn(branch_1, 32, 3, name=name_fmt('Conv2d_0b_3x3', 1))\n",
    "        branch_2 = conv2d_bn(x, 32, 1, name=name_fmt('Conv2d_0a_1x1', 2))\n",
    "        branch_2 = conv2d_bn(branch_2, 32, 3, name=name_fmt('Conv2d_0b_3x3', 2))\n",
    "        branch_2 = conv2d_bn(branch_2, 32, 3, name=name_fmt('Conv2d_0c_3x3', 2))\n",
    "        branches = [branch_0, branch_1, branch_2]\n",
    "    elif block_type == 'Block17':\n",
    "        branch_0 = conv2d_bn(x, 128, 1, name=name_fmt('Conv2d_1x1', 0))\n",
    "        branch_1 = conv2d_bn(x, 128, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n",
    "        branch_1 = conv2d_bn(branch_1, 128, [1, 7], name=name_fmt('Conv2d_0b_1x7', 1))\n",
    "        branch_1 = conv2d_bn(branch_1, 128, [7, 1], name=name_fmt('Conv2d_0c_7x1', 1))\n",
    "        branches = [branch_0, branch_1]\n",
    "    elif block_type == 'Block8':\n",
    "        branch_0 = conv2d_bn(x, 192, 1, name=name_fmt('Conv2d_1x1', 0))\n",
    "        branch_1 = conv2d_bn(x, 192, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n",
    "        branch_1 = conv2d_bn(branch_1, 192, [1, 3], name=name_fmt('Conv2d_0b_1x3', 1))\n",
    "        branch_1 = conv2d_bn(branch_1, 192, [3, 1], name=name_fmt('Conv2d_0c_3x1', 1))\n",
    "        branches = [branch_0, branch_1]\n",
    "    else:\n",
    "        raise ValueError('Unknown Inception-ResNet block type. '\n",
    "                         'Expects \"Block35\", \"Block17\" or \"Block8\", '\n",
    "                         'but got: ' + str(block_type))\n",
    "\n",
    "    mixed = Concatenate(axis=channel_axis, name=name_fmt('Concatenate'))(branches)\n",
    "    up = conv2d_bn(mixed,\n",
    "                   K.int_shape(x)[channel_axis],\n",
    "                   1,\n",
    "                   activation=None,\n",
    "                   use_bias=True,\n",
    "                   name=name_fmt('Conv2d_1x1'))\n",
    "    up = Lambda(scaling,\n",
    "                output_shape=K.int_shape(up)[1:],\n",
    "                arguments={'scale': scale})(up)\n",
    "    x = add([x, up])\n",
    "    if activation is not None:\n",
    "        x = Activation(activation, name=name_fmt('Activation'))(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced from https://github.com/nyoki-mtl/keras-facenet/blob/master/code/inception_resnet_v1.py\n",
    "\n",
    "def InceptionResNetV1(input_shape=(160, 160, 3),\n",
    "                      classes=128,\n",
    "                      dropout_keep_prob=0.8,\n",
    "                      weights_path=None):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = conv2d_bn(inputs, 32, 3, strides=2, padding='valid', name='Conv2d_1a_3x3')\n",
    "    x = conv2d_bn(x, 32, 3, padding='valid', name='Conv2d_2a_3x3')\n",
    "    x = conv2d_bn(x, 64, 3, name='Conv2d_2b_3x3')\n",
    "    x = MaxPooling2D(3, strides=2, name='MaxPool_3a_3x3')(x)\n",
    "    x = conv2d_bn(x, 80, 1, padding='valid', name='Conv2d_3b_1x1')\n",
    "    x = conv2d_bn(x, 192, 3, padding='valid', name='Conv2d_4a_3x3')\n",
    "    x = conv2d_bn(x, 256, 3, strides=2, padding='valid', name='Conv2d_4b_3x3')\n",
    "\n",
    "    # 5x Block35 (Inception-ResNet-A block):\n",
    "    for block_idx in range(1, 6):\n",
    "        x = _inception_resnet_block(x,\n",
    "                                    scale=0.17,\n",
    "                                    block_type='Block35',\n",
    "                                    block_idx=block_idx)\n",
    "\n",
    "    # Mixed 6a (Reduction-A block):\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else 3\n",
    "    name_fmt = partial(_generate_layer_name, prefix='Mixed_6a')\n",
    "    branch_0 = conv2d_bn(x,\n",
    "                         384,\n",
    "                         3,\n",
    "                         strides=2,\n",
    "                         padding='valid',\n",
    "                         name=name_fmt('Conv2d_1a_3x3', 0))\n",
    "    branch_1 = conv2d_bn(x, 192, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n",
    "    branch_1 = conv2d_bn(branch_1, 192, 3, name=name_fmt('Conv2d_0b_3x3', 1))\n",
    "    branch_1 = conv2d_bn(branch_1,\n",
    "                         256,\n",
    "                         3,\n",
    "                         strides=2,\n",
    "                         padding='valid',\n",
    "                         name=name_fmt('Conv2d_1a_3x3', 1))\n",
    "    branch_pool = MaxPooling2D(3,\n",
    "                               strides=2,\n",
    "                               padding='valid',\n",
    "                               name=name_fmt('MaxPool_1a_3x3', 2))(x)\n",
    "    branches = [branch_0, branch_1, branch_pool]\n",
    "    x = Concatenate(axis=channel_axis, name='Mixed_6a')(branches)\n",
    "\n",
    "    # 10x Block17 (Inception-ResNet-B block):\n",
    "    for block_idx in range(1, 11):\n",
    "        x = _inception_resnet_block(x,\n",
    "                                    scale=0.1,\n",
    "                                    block_type='Block17',\n",
    "                                    block_idx=block_idx)\n",
    "\n",
    "    # Mixed 7a (Reduction-B block): 8 x 8 x 2080\n",
    "    name_fmt = partial(_generate_layer_name, prefix='Mixed_7a')\n",
    "    branch_0 = conv2d_bn(x, 256, 1, name=name_fmt('Conv2d_0a_1x1', 0))\n",
    "    branch_0 = conv2d_bn(branch_0,\n",
    "                         384,\n",
    "                         3,\n",
    "                         strides=2,\n",
    "                         padding='valid',\n",
    "                         name=name_fmt('Conv2d_1a_3x3', 0))\n",
    "    branch_1 = conv2d_bn(x, 256, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n",
    "    branch_1 = conv2d_bn(branch_1,\n",
    "                         256,\n",
    "                         3,\n",
    "                         strides=2,\n",
    "                         padding='valid',\n",
    "                         name=name_fmt('Conv2d_1a_3x3', 1))\n",
    "    branch_2 = conv2d_bn(x, 256, 1, name=name_fmt('Conv2d_0a_1x1', 2))\n",
    "    branch_2 = conv2d_bn(branch_2, 256, 3, name=name_fmt('Conv2d_0b_3x3', 2))\n",
    "    branch_2 = conv2d_bn(branch_2,\n",
    "                         256,\n",
    "                         3,\n",
    "                         strides=2,\n",
    "                         padding='valid',\n",
    "                         name=name_fmt('Conv2d_1a_3x3', 2))\n",
    "    branch_pool = MaxPooling2D(3,\n",
    "                               strides=2,\n",
    "                               padding='valid',\n",
    "                               name=name_fmt('MaxPool_1a_3x3', 3))(x)\n",
    "    branches = [branch_0, branch_1, branch_2, branch_pool]\n",
    "    x = Concatenate(axis=channel_axis, name='Mixed_7a')(branches)\n",
    "\n",
    "    # 5x Block8 (Inception-ResNet-C block):\n",
    "    for block_idx in range(1, 6):\n",
    "        x = _inception_resnet_block(x,\n",
    "                                    scale=0.2,\n",
    "                                    block_type='Block8',\n",
    "                                    block_idx=block_idx)\n",
    "    x = _inception_resnet_block(x,\n",
    "                                scale=1.,\n",
    "                                activation=None,\n",
    "                                block_type='Block8',\n",
    "                                block_idx=6)\n",
    "\n",
    "    # Classification block\n",
    "    x = GlobalAveragePooling2D(name='AvgPool')(x)\n",
    "    x = Dropout(1.0 - dropout_keep_prob, name='Dropout')(x)\n",
    "    # Bottleneck\n",
    "    x = Dense(classes, use_bias=False, name='Bottleneck')(x)\n",
    "    bn_name = _generate_layer_name('BatchNorm', prefix='Bottleneck')\n",
    "    x = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False,\n",
    "                           name=bn_name)(x)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs, x, name='inception_resnet_v1')\n",
    "    if weights_path is not None:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referenced from https://github.com/davidsandberg/facenet/blob/master/src/train_tripletloss.py\n",
    "def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\n",
    "    \"\"\" Select the triplets for training\n",
    "    \"\"\"\n",
    "    trip_idx = 0\n",
    "    emb_start_idx = 0\n",
    "    num_trips = 0\n",
    "    triplets = []\n",
    "\n",
    "    # VGG Face: Choosing good triplets is crucial and should strike a balance between\n",
    "    #  selecting informative (i.e. challenging) examples and swamping training with examples that\n",
    "    #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\n",
    "    #  the image n at random, but only between the ones that violate the triplet loss margin. The\n",
    "    #  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\n",
    "    #  choosing the maximally violating example, as often done in structured output learning.\n",
    "\n",
    "    for i in xrange(people_per_batch):\n",
    "        nrof_images = int(nrof_images_per_class[i])\n",
    "        for j in xrange(1, nrof_images):\n",
    "            a_idx = emb_start_idx + j - 1\n",
    "            neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\n",
    "            for pair in xrange(j, nrof_images):  # For every possible positive pair.\n",
    "                p_idx = emb_start_idx + pair\n",
    "                pos_dist_sqr = np.sum(np.square(embeddings[a_idx] - embeddings[p_idx]))\n",
    "                neg_dists_sqr[emb_start_idx:emb_start_idx + nrof_images] = np.NaN\n",
    "                # all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\n",
    "                all_neg = np.where(neg_dists_sqr - pos_dist_sqr < alpha)[0]  # VGG Face selecction\n",
    "                nrof_random_negs = all_neg.shape[0]\n",
    "                if nrof_random_negs > 0:\n",
    "                    rnd_idx = np.random.randint(nrof_random_negs)\n",
    "                    n_idx = all_neg[rnd_idx]\n",
    "                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\n",
    "                    # print('Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)' %\n",
    "                    #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\n",
    "                    trip_idx += 1\n",
    "\n",
    "                num_trips += 1\n",
    "\n",
    "        emb_start_idx += nrof_images\n",
    "\n",
    "    np.random.shuffle(triplets)\n",
    "    return triplets, num_trips, len(triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referenced from https://github.com/davidsandberg/facenet/blob/master/src/train_tripletloss.py\n",
    "\n",
    "def sample_people(dataset, people_per_batch, images_per_person):\n",
    "    nrof_images = people_per_batch * images_per_person\n",
    "\n",
    "    # Sample classes from the dataset\n",
    "    nrof_classes = len(dataset)\n",
    "    class_indices = np.arange(nrof_classes)\n",
    "    np.random.shuffle(class_indices)\n",
    "\n",
    "    i = 0\n",
    "    image_paths = []\n",
    "    num_per_class = []\n",
    "    sampled_class_indices = []\n",
    "    # Sample images from these classes until we have enough\n",
    "    while len(image_paths) < nrof_images:\n",
    "        class_index = class_indices[i]\n",
    "        nrof_images_in_class = len(dataset[class_index])\n",
    "        image_indices = np.arange(nrof_images_in_class)\n",
    "        np.random.shuffle(image_indices)\n",
    "        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images - len(image_paths))\n",
    "        idx = image_indices[0:nrof_images_from_class]\n",
    "        image_paths_for_class = [dataset[class_index][j] for j in idx]\n",
    "        sampled_class_indices += [class_index] * nrof_images_from_class\n",
    "        image_paths += image_paths_for_class\n",
    "        num_per_class.append(nrof_images_from_class)\n",
    "        i += 1\n",
    "\n",
    "    return image_paths, num_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my own implementation triplet_loss (from FaceNet paper)\n",
    "def triplet_loss_keras(y_true, y_pred, alpha = 0.2):\n",
    "    batch_size = y_pred.shape[0]\n",
    "    anchor, positive, negative = y_pred[0::3], y_pred[1::3], y_pred[2::3]\n",
    "    #Compute the (encoding) distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)), axis=-1)\n",
    "    # Compute the (encoding) distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)), axis=-1)\n",
    "    # subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)\n",
    "    # Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced from https://github.com/davidsandberg/facenet/blob/master/src/facenet.py\n",
    "def get_image_paths(facedir):\n",
    "    image_paths = []\n",
    "    if os.path.isdir(facedir):\n",
    "        images = os.listdir(facedir)\n",
    "        image_paths = [os.path.join(facedir,img) for img in images]\n",
    "    return image_paths\n",
    "\n",
    "# Referenced from https://github.com/davidsandberg/facenet/blob/master/src/facenet.py\n",
    "def get_dataset(path, has_class_directories=True):\n",
    "    dataset = []\n",
    "    path_exp = os.path.expanduser(path)\n",
    "    classes = [path for path in os.listdir(path_exp) \\\n",
    "               if os.path.isdir(os.path.join(path_exp, path))]\n",
    "    classes.sort()\n",
    "    nrof_classes = len(classes)\n",
    "    for i in range(nrof_classes):\n",
    "        class_name = classes[i]\n",
    "        facedir = os.path.join(path_exp, class_name)\n",
    "        image_paths = get_image_paths(facedir)\n",
    "        dataset.append(image_paths)\n",
    "    return dataset\n",
    "\n",
    "# my own implementation to read images from directory\n",
    "def get_images(image_paths):\n",
    "    h, w, c = 160, 160, 3\n",
    "    num_of_images = len(image_paths)\n",
    "    images = np.zeros((num_of_images, h, w, c))\n",
    "    for i, path in enumerate(image_paths):\n",
    "        img = Image.open(path)\n",
    "        img = img.resize((w, h))\n",
    "        images[i, :, :, :] = img\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "weights_path = '../facenet_keras_weights.h5'\n",
    "model = InceptionResNetV1(weights_path=weights_path)\n",
    "# model.layers.pop()\n",
    "# # tf.(model.layers[-1].output)\n",
    "l2_norm = Lambda(lambda  x: K.l2_normalize(x,axis=1))(model.layers[-1].output)\n",
    "\n",
    "model_new  = Model(input=model.input, output=[l2_norm])\n",
    "model_new.compile(optimizer='adam',\n",
    "              loss=triplet_loss_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "dataset_path = '../Dataset'\n",
    "dataset = get_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of triplets1740\n",
      "16.800278\n",
      "25.269215\n",
      "21.029514\n",
      "10.938055\n",
      "10.705985\n",
      "19.512146\n",
      "6.995944\n",
      "11.992165\n",
      "20.266356\n",
      "14.867696\n",
      "9.2652445\n",
      "8.8151865\n",
      "14.564001\n",
      "15.179854\n",
      "8.183182\n",
      "6.578458\n",
      "6.299381\n",
      "9.092476\n",
      "12.003678\n",
      "3.5901418\n",
      "3.5730906\n",
      "7.414133\n",
      "12.322477\n",
      "2.057243\n",
      "3.9045613\n",
      "1.9242547\n",
      "4.5549917\n",
      "11.746856\n",
      "9.941087\n",
      "9.722772\n",
      "11.229495\n",
      "4.722591\n",
      "7.2852883\n",
      "2.0447068\n",
      "10.827441\n",
      "6.00235\n",
      "1.6357831\n",
      "0.79121155\n",
      "0.847751\n",
      "3.2333302\n",
      "3.1081064\n",
      "2.5763242\n",
      "9.058413\n",
      "2.618049\n",
      "2.348679\n",
      "1.6696794\n",
      "4.0799336\n",
      "3.859984\n",
      "1.6308553\n",
      "2.7144086\n",
      "1.232973\n",
      "5.5552025\n",
      "4.175487\n",
      "5.5458083\n",
      "3.8896017\n",
      "3.6499085\n",
      "3.2988915\n",
      "5.138017\n",
      "number of triplets1738\n",
      "3.9954314\n",
      "27.790525\n",
      "23.399431\n",
      "24.838322\n",
      "16.359358\n",
      "15.399206\n",
      "16.319513\n",
      "13.47087\n",
      "7.608561\n",
      "11.786734\n",
      "5.383022\n",
      "15.884826\n",
      "16.65341\n",
      "9.736891\n",
      "13.091261\n",
      "9.255256\n",
      "20.878756\n",
      "14.172364\n",
      "11.135842\n",
      "14.624121\n",
      "11.530351\n",
      "9.616345\n",
      "6.9255514\n",
      "12.137709\n",
      "8.34192\n",
      "9.409983\n",
      "19.02843\n",
      "10.818833\n",
      "13.470378\n",
      "4.369876\n",
      "8.200716\n",
      "8.473695\n",
      "9.380118\n",
      "7.884843\n",
      "9.965869\n",
      "5.925012\n",
      "8.03457\n",
      "7.450081\n",
      "3.8423207\n",
      "3.1364956\n",
      "7.91965\n",
      "5.2864347\n",
      "6.7086554\n",
      "2.1951466\n",
      "4.446524\n",
      "4.1078887\n",
      "4.796456\n",
      "5.5430064\n",
      "3.1004553\n",
      "1.4679657\n",
      "4.218437\n",
      "10.001212\n",
      "3.0044248\n",
      "3.0911374\n",
      "1.9947851\n",
      "5.667267\n",
      "9.412917\n",
      "1.1302195\n",
      "number of triplets1735\n",
      "14.991613\n",
      "8.659668\n",
      "25.197016\n",
      "8.711157\n",
      "12.395556\n",
      "6.101399\n",
      "8.483772\n",
      "5.156528\n",
      "8.115465\n",
      "12.238127\n",
      "11.329241\n",
      "5.012314\n",
      "10.162881\n",
      "10.95169\n",
      "10.767603\n",
      "6.7482786\n",
      "5.3694386\n",
      "7.373841\n",
      "2.808851\n",
      "11.0676565\n",
      "13.569754\n",
      "3.8606486\n",
      "14.010492\n",
      "6.869102\n",
      "4.970674\n",
      "7.7231064\n",
      "9.824304\n",
      "7.5857015\n",
      "7.2171373\n",
      "5.712772\n",
      "4.1538396\n",
      "6.92299\n",
      "5.1504626\n",
      "2.5359266\n",
      "7.146982\n",
      "2.4102333\n",
      "3.6539168\n",
      "4.1167374\n",
      "5.633281\n",
      "3.325687\n",
      "3.8345914\n",
      "3.6375926\n",
      "2.3781488\n",
      "6.9824224\n",
      "4.088982\n",
      "5.3511596\n",
      "3.5243337\n",
      "3.897224\n",
      "3.3448958\n",
      "3.6359391\n",
      "3.7788272\n",
      "4.2131276\n",
      "2.3662076\n",
      "2.782667\n",
      "4.193482\n",
      "5.420006\n",
      "3.780023\n",
      "0.21098389\n",
      "number of triplets1740\n",
      "9.3161745\n",
      "17.268072\n",
      "11.60773\n",
      "12.82434\n",
      "8.453476\n",
      "7.3781133\n",
      "7.122096\n",
      "6.845025\n",
      "8.355573\n",
      "11.586149\n",
      "6.4108114\n",
      "7.2590265\n",
      "9.491806\n",
      "9.9150915\n",
      "9.5996065\n",
      "7.2045527\n",
      "7.042694\n",
      "3.3469698\n",
      "9.043061\n",
      "4.041782\n",
      "4.0485225\n",
      "7.640937\n",
      "3.504673\n",
      "6.3856363\n",
      "5.852436\n",
      "3.6995578\n",
      "4.408458\n",
      "4.2040696\n",
      "4.767959\n",
      "4.092177\n",
      "4.032048\n",
      "5.886977\n",
      "5.9399676\n",
      "3.4808848\n",
      "7.3735275\n",
      "2.9671106\n",
      "3.2034357\n",
      "3.7727733\n",
      "3.7715793\n",
      "4.8918824\n",
      "1.5888479\n",
      "2.559725\n",
      "7.4647517\n",
      "0.8883973\n",
      "6.5809627\n",
      "4.7434273\n",
      "3.620319\n",
      "5.9507\n",
      "2.1129043\n",
      "2.350842\n",
      "3.6692631\n",
      "5.696075\n",
      "5.455528\n",
      "1.593152\n",
      "1.242213\n",
      "3.0309076\n",
      "3.3673093\n",
      "2.2371502\n",
      "number of triplets1737\n",
      "2.9137917\n",
      "5.791482\n",
      "3.9739413\n",
      "2.1629336\n",
      "4.6479216\n",
      "2.7015548\n",
      "2.9934695\n",
      "4.649362\n",
      "2.8247626\n",
      "3.23827\n",
      "3.2602515\n",
      "2.7572253\n",
      "3.180306\n",
      "1.8830413\n",
      "4.278775\n",
      "3.5166833\n",
      "2.7951052\n",
      "0.94876707\n",
      "2.2572653\n",
      "3.886196\n",
      "4.9101024\n",
      "5.2457194\n",
      "6.9826193\n",
      "2.0504646\n",
      "4.318829\n",
      "1.4107999\n",
      "4.746117\n",
      "2.5727308\n",
      "2.2998636\n",
      "1.7145779\n",
      "3.638681\n",
      "3.822659\n",
      "4.884571\n",
      "2.7475984\n",
      "3.681169\n",
      "4.4137607\n",
      "1.5266398\n",
      "1.856809\n",
      "4.0212646\n",
      "1.8470784\n",
      "2.6741388\n",
      "0.28398275\n",
      "5.1768336\n",
      "3.0884655\n",
      "0.9946171\n",
      "1.5147766\n",
      "3.5210114\n",
      "1.2436591\n",
      "1.8255978\n",
      "0.8142461\n",
      "1.1488006\n",
      "1.1707062\n",
      "2.4229906\n",
      "2.773115\n",
      "0.49357474\n",
      "0.93937296\n",
      "0.97152865\n",
      "0.5514917\n",
      "number of triplets1740\n",
      "10.208245\n",
      "6.0964465\n",
      "5.134346\n",
      "7.5387545\n",
      "8.571826\n",
      "5.3060308\n",
      "5.2142606\n",
      "4.186868\n",
      "5.303948\n",
      "6.438395\n",
      "3.9034421\n",
      "4.723557\n",
      "6.1528482\n",
      "2.8776686\n",
      "3.127713\n",
      "7.084786\n",
      "3.0628886\n",
      "4.72023\n",
      "11.808937\n",
      "5.6681023\n",
      "7.488864\n",
      "3.28262\n",
      "6.5805745\n",
      "7.732386\n",
      "5.425119\n",
      "5.685318\n",
      "11.434426\n",
      "7.5744376\n",
      "7.88573\n",
      "12.75525\n",
      "8.518509\n",
      "7.9108133\n",
      "6.122193\n",
      "5.3262625\n",
      "3.359799\n",
      "8.650886\n",
      "6.276269\n",
      "7.7644625\n",
      "4.578613\n",
      "6.5604715\n",
      "8.094811\n",
      "5.4808116\n",
      "4.438805\n",
      "7.7842875\n",
      "4.59856\n",
      "4.486622\n",
      "8.286547\n",
      "8.709624\n",
      "7.0001073\n",
      "10.789772\n",
      "6.0110283\n",
      "3.508499\n",
      "7.020322\n",
      "2.6159592\n",
      "3.2477472\n",
      "6.5440288\n",
      "4.8483396\n",
      "5.3643465\n",
      "number of triplets1740\n",
      "7.7178245\n",
      "13.740157\n",
      "8.734353\n",
      "6.8482924\n",
      "11.182396\n",
      "7.4440737\n",
      "7.6413674\n",
      "12.631896\n",
      "6.7543335\n",
      "11.169076\n",
      "10.045592\n",
      "18.032501\n",
      "12.153292\n",
      "13.032974\n",
      "12.388816\n",
      "9.146446\n",
      "12.825834\n",
      "6.3167806\n",
      "8.6977\n",
      "9.611419\n",
      "7.515998\n",
      "10.149032\n",
      "11.944703\n",
      "5.588589\n",
      "6.009648\n",
      "6.8397093\n",
      "5.905272\n",
      "5.7249975\n",
      "5.4647136\n",
      "8.949502\n",
      "8.733902\n",
      "9.17753\n",
      "7.201703\n",
      "8.345277\n",
      "9.104957\n",
      "3.3803277\n",
      "8.783622\n",
      "6.4544454\n",
      "7.5810194\n",
      "3.7203736\n",
      "10.530669\n",
      "5.9861565\n",
      "5.2319407\n",
      "5.465404\n",
      "3.2194784\n",
      "10.34294\n",
      "6.677903\n",
      "3.6567292\n",
      "4.563919\n",
      "7.6981077\n",
      "3.7441304\n",
      "3.3502374\n",
      "11.562606\n",
      "4.6236024\n",
      "6.9703145\n",
      "4.0622144\n",
      "2.2293742\n",
      "4.108252\n",
      "number of triplets1740\n",
      "8.647956\n",
      "10.467246\n",
      "9.072052\n",
      "7.361474\n",
      "17.95941\n",
      "5.433168\n",
      "6.1151204\n",
      "8.853775\n",
      "5.404816\n",
      "5.042371\n",
      "5.6743507\n",
      "5.7402034\n",
      "10.777261\n",
      "10.162675\n",
      "11.771422\n",
      "7.0068846\n",
      "6.5124483\n",
      "8.084429\n",
      "6.8902583\n",
      "7.7782693\n",
      "4.807344\n",
      "7.0176325\n",
      "8.720352\n",
      "8.554931\n",
      "7.804553\n",
      "8.478057\n",
      "4.337052\n",
      "5.5334926\n",
      "7.1996684\n",
      "4.8854175\n",
      "7.7958746\n",
      "8.855232\n",
      "7.523658\n",
      "8.310975\n",
      "6.794151\n",
      "7.523921\n",
      "7.973763\n",
      "7.878624\n",
      "5.4637923\n",
      "8.861421\n",
      "7.931836\n",
      "5.7129827\n",
      "7.4787035\n",
      "9.454133\n",
      "12.172153\n",
      "8.763131\n",
      "6.208157\n",
      "7.7679415\n",
      "8.587908\n",
      "10.619971\n",
      "8.251342\n",
      "8.574791\n",
      "8.677644\n",
      "8.824232\n",
      "4.5172615\n",
      "6.053626\n",
      "11.041598\n",
      "5.779533\n",
      "number of triplets1740\n",
      "11.661287\n",
      "9.032273\n",
      "4.518635\n",
      "7.9415283\n",
      "4.625503\n",
      "4.6306767\n",
      "4.4132166\n",
      "6.8408427\n",
      "4.102569\n",
      "7.5855455\n",
      "6.594865\n",
      "4.8306684\n",
      "9.469902\n",
      "4.031879\n",
      "4.4232116\n",
      "6.54013\n",
      "5.092094\n",
      "5.0582957\n",
      "5.8274646\n",
      "6.036611\n",
      "7.9282527\n",
      "5.1991315\n",
      "7.1795554\n",
      "9.0792055\n",
      "7.660319\n",
      "7.0958457\n",
      "7.9392085\n",
      "6.0937967\n",
      "4.0367246\n",
      "3.1117897\n",
      "7.743087\n",
      "4.0738597\n",
      "3.0677474\n",
      "6.884822\n",
      "3.1030347\n",
      "6.0399365\n",
      "5.476418\n",
      "3.682316\n",
      "2.8281271\n",
      "9.779572\n",
      "11.439318\n",
      "2.9857364\n",
      "5.235779\n",
      "9.577798\n",
      "2.154757\n",
      "3.6021914\n",
      "0.65894085\n",
      "8.156869\n",
      "4.6985865\n",
      "4.6843944\n",
      "6.5131125\n",
      "5.58837\n",
      "6.6569295\n",
      "4.99184\n",
      "1.511595\n",
      "2.20997\n",
      "2.673654\n",
      "4.405154\n",
      "number of triplets1740\n",
      "9.161591\n",
      "5.139932\n",
      "4.485083\n",
      "4.7522874\n",
      "8.366235\n",
      "4.9875565\n",
      "7.4192877\n",
      "8.100207\n",
      "3.8653493\n",
      "5.5334196\n",
      "6.7210584\n",
      "12.328741\n",
      "6.9981084\n",
      "7.319031\n",
      "5.3302\n",
      "4.3679314\n",
      "6.0166545\n",
      "7.0488505\n",
      "3.9640462\n",
      "4.382564\n",
      "8.5199795\n",
      "6.686755\n",
      "4.495256\n",
      "4.8928146\n",
      "5.222214\n",
      "3.3575022\n",
      "6.2725086\n",
      "6.8694987\n",
      "3.6181986\n",
      "4.106583\n",
      "5.6449623\n",
      "4.891\n",
      "3.2203825\n",
      "7.37972\n",
      "8.045097\n",
      "3.792397\n",
      "3.8993192\n",
      "2.3728433\n",
      "3.3236299\n",
      "3.093063\n",
      "2.3475153\n",
      "6.799907\n",
      "4.0657496\n",
      "5.952541\n",
      "5.9451675\n",
      "2.149659\n",
      "2.4407234\n",
      "3.1564157\n",
      "3.277418\n",
      "6.159075\n",
      "2.7738068\n",
      "2.8572788\n",
      "5.507168\n",
      "2.786812\n",
      "3.3366754\n",
      "8.202989\n",
      "4.4388976\n",
      "8.607018\n"
     ]
    }
   ],
   "source": [
    "model = model_new\n",
    "people_per_batch, images_per_person, batch_size, embedding_size, alpha = 4, 30, 30, 128, 0.2\n",
    "for epoch in range(epochs):\n",
    "    image_paths, num_per_class = sample_people(dataset, people_per_batch, images_per_person)\n",
    "    nrof_examples = people_per_batch * images_per_person\n",
    "    emb_array = np.zeros((nrof_examples, embedding_size))\n",
    "    nrof_batches = int(np.ceil(nrof_examples / batch_size))\n",
    "    for i in range(nrof_batches):\n",
    "        start = i * batch_size\n",
    "        local_batch_size = min(nrof_examples - i * batch_size, batch_size)\n",
    "        end = start + local_batch_size\n",
    "        predictions = model.predict(get_images(image_paths[start:end]))\n",
    "        emb_array[start:end, :] = predictions\n",
    "    triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class,\n",
    "                                                               image_paths, people_per_batch, alpha)\n",
    "    print(\"number of triplets\"+str(nrof_triplets))\n",
    "    nrof_batches = int(np.ceil(nrof_triplets/batch_size))\n",
    "    for i in range(nrof_batches):\n",
    "        start = i * batch_size\n",
    "        local_batch_size = min(nrof_triplets - i * batch_size, batch_size)\n",
    "        end = start + local_batch_size\n",
    "        image_paths_triplets = triplets[start:end]\n",
    "        image_paths_mini = [image_path for triples in image_paths_triplets for image_path in triples]\n",
    "        num_of_images = len(image_paths)\n",
    "        x = get_images(image_paths_mini)\n",
    "        y_dummy = np.random.rand(local_batch_size*3, 128)\n",
    "        print(model.train_on_batch(x, y_dummy, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced from deeplearning.ai's cnn course\n",
    "def img_to_encoding(image_path, model):\n",
    "    img1 = cv2.imread(image_path, 1)\n",
    "    img1= cv2.resize(img1, (160, 160))\n",
    "    img = img1[...,::-1]\n",
    "    #img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced from deeplearning.ai's cnn course\n",
    "# Face Verification task\n",
    "# verify function measures the distance between new image encoding and encoding value with key value as person's name\n",
    "def verify(image_path, identity, database, model):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    image_path -- path to an image\n",
    "    identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house.\n",
    "    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Output:  \n",
    "    dist -- distance between the image_path and the image of \"identity\" in the database.\n",
    "    door_open -- True, if the door should open. False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute the encoding for the image.\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    \n",
    "    # Compute distance with identity's image\n",
    "    dist = np.linalg.norm(encoding-database[identity])\n",
    "    \n",
    "    # Open the door if dist < 0.7, else don't open\n",
    "    if dist<0.7:\n",
    "        print(\"It's \" + str(identity) + \", welcome home!\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"It's not \" + str(identity) + \", please go away\")\n",
    "        door_open = False\n",
    "        \n",
    "        \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced from deeplearning.ai's cnn course\n",
    "\n",
    "# Face recognitio task\n",
    "# In this task, new image encodings are matched with all the existing encodings and the encoding with minimum difference,\n",
    "# less than threshold value will be selected as similar identity and Functionfwill return the respective key value\n",
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the happy house by finding who is the person on the image_path image.\n",
    "    \n",
    "    Input:\n",
    "    image_path -- path to an image\n",
    "    database -- database containing image encodings along with the name of the person on the image\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Output:\n",
    "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "    identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #Compute the target \"encoding\" for the image\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    \n",
    "    # Initialize \"min_dist\" to a large value, say 100\n",
    "    min_dist = 100\n",
    "    \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database.\n",
    "        dist = np.linalg.norm(encoding-database[name])\n",
    "\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name.\n",
    "        if dist<min_dist:\n",
    "            min_dist = dist\n",
    "            identity = namek\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
